# -*- coding: utf-8 -*-
"""ASL_Fine_Tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FsfjMW8Wu8oR18_YDsTkCU7F_hHndCPK

# ASL Alphabet Recognition: WLASL Letter Fine-Tuning Pipeline

This notebook fine-tunes an existing ResNet-18 ASL alphabet classifier with letter-only samples from the WLASL dataset to improve robustness to real-world webcam conditions.

## Pipeline Overview:
1. Extract letter frames from WLASL videos
2. Apply MediaPipe hand detection and cropping
3. Mix with clean ASL Alphabet dataset
4. Fine-tune existing model
5. Evaluate improvements

**Important:** This assumes you already have a trained model (`asl_resnet18_best.pth`) from the clean ASL Alphabet dataset.

## 1. Setup and Installations
"""

# Install required packages
!pip install -q mediapipe kagglehub opencv-python-headless

print("âœ“ Packages installed")

# Import libraries
import os
import cv2
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from torchvision.datasets import ImageFolder
from pathlib import Path
from collections import defaultdict
from tqdm.auto import tqdm
import mediapipe as mp
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

print("âœ“ Libraries imported")

# Configuration
IMAGE_SIZE = 224
NUM_CLASSES = 29
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Training settings
BATCH_SIZE = 64
LEARNING_RATE = 1e-4
WEIGHT_DECAY = 1e-4
FINETUNE_EPOCHS = 10
NUM_WORKERS = 2

# Dataset mixing
CLEAN_RATIO = 0.65  # 65% clean, 35% noisy

# Frame extraction
FRAME_INTERVAL = 4  # Extract every 4th frame
MAX_FRAMES_PER_VIDEO = 20

print(f"âœ“ Configuration set")
print(f"  Device: {DEVICE}")
print(f"  Batch size: {BATCH_SIZE}")
print(f"  Clean/Noisy ratio: {CLEAN_RATIO:.0%}/{1-CLEAN_RATIO:.0%}")

"""## 2. Download Datasets"""

import kagglehub

# Download ASL Alphabet dataset (clean)
print("Downloading ASL Alphabet dataset...")
grassknoted_asl_alphabet_path = kagglehub.dataset_download('grassknoted/asl-alphabet')
print(f"âœ“ ASL Alphabet path: {grassknoted_asl_alphabet_path}")

# Download WLASL dataset (noisy, real-world)
print("\nDownloading WLASL dataset...")
wlasl_path = kagglehub.dataset_download('waseemnagahhenes/sign-language-dataset-wlasl-videos')
print(f"âœ“ WLASL path: {wlasl_path}")

print("\nâœ“ All datasets downloaded")

"""## 3. WLASL Letter Frame Extraction"""

class WLASLLetterExtractor:
    """Extract letter-only frames from WLASL videos"""

    LETTER_GLOSSES = set('ABCDEFGHIJKLMNOPQRSTUVWXYZ')

    def __init__(self, wlasl_root, frame_interval=4):
        self.wlasl_root = Path(wlasl_root)
        self.frame_interval = frame_interval

    def find_letter_videos(self):
        """Scan WLASL directory structure for letter videos"""
        letter_videos = []

        # Try common WLASL directory structures
        possible_dirs = [
            self.wlasl_root / 'videos',
            self.wlasl_root / 'WLASL2000',
            self.wlasl_root,
        ]

        videos_dir = None
        for dir_path in possible_dirs:
            if dir_path.exists():
                videos_dir = dir_path
                break

        if videos_dir is None:
            print("âš  Could not find videos directory")
            return []

        print(f"Scanning: {videos_dir}")

        # Look for letter subdirectories
        for item in videos_dir.iterdir():
            if not item.is_dir():
                continue

            gloss = item.name.upper()

            # Only process single-letter glosses
            if gloss in self.LETTER_GLOSSES:
                for video_path in item.glob('*.mp4'):
                    letter_videos.append({
                        'video_path': video_path,
                        'label': gloss
                    })

        return letter_videos

    def extract_frames_from_video(self, video_path, max_frames=20):
        """Extract frames from middle 60% of video (avoid transitions)"""
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            return []

        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # Skip first and last 20% to avoid hand entering/leaving frame
        start_frame = int(total_frames * 0.2)
        end_frame = int(total_frames * 0.8)

        frames = []
        frame_idx = 0

        while cap.isOpened() and len(frames) < max_frames:
            ret, frame = cap.read()
            if not ret:
                break

            # Extract from stable middle portion
            if start_frame <= frame_idx <= end_frame:
                if (frame_idx - start_frame) % self.frame_interval == 0:
                    frames.append(frame)

            frame_idx += 1

        cap.release()
        return frames

    def process_all(self, output_dir, max_frames_per_video=20):
        """Extract all letter frames and save to disk"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        letter_videos = self.find_letter_videos()

        if not letter_videos:
            print("âš  No letter videos found. Check WLASL directory structure.")
            return {}

        stats = defaultdict(int)

        print(f"Found {len(letter_videos)} letter videos. Extracting frames...")

        for video_info in tqdm(letter_videos, desc="Extracting frames"):
            label = video_info['label']
            video_path = video_info['video_path']

            frames = self.extract_frames_from_video(video_path, max_frames_per_video)

            if not frames:
                continue

            # Save frames
            label_dir = output_dir / label
            label_dir.mkdir(exist_ok=True)

            video_id = video_path.stem
            for idx, frame in enumerate(frames):
                frame_name = f"{video_id}_frame{idx:03d}.jpg"
                cv2.imwrite(str(label_dir / frame_name), frame)
                stats[label] += 1

        print("\nâœ“ Frame extraction complete:")
        for letter in sorted(stats.keys()):
            print(f"  {letter}: {stats[letter]} frames")

        return stats

print("âœ“ WLASLLetterExtractor class defined")

# Extract letter frames from WLASL
extractor = WLASLLetterExtractor(wlasl_path, frame_interval=FRAME_INTERVAL)
extraction_stats = extractor.process_all(
    output_dir='/content/wlasl_letters_raw',
    max_frames_per_video=MAX_FRAMES_PER_VIDEO
)

"""## 4. MediaPipe Hand Detection & Cropping"""

class MediaPipeHandCropper:
    """Apply MediaPipe hand detection + cropping (matches inference pipeline)"""

    def __init__(self, image_size=224, padding_ratio=0.15):
        self.mp_hands = mp.solutions.hands.Hands(
            static_image_mode=True,
            max_num_hands=1,
            min_detection_confidence=0.5
        )
        self.image_size = image_size
        self.padding_ratio = padding_ratio

    def detect_and_crop(self, image):
        """Detect hand and return square crop (PIL Image)"""
        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w = rgb.shape[:2]

        results = self.mp_hands.process(rgb)

        if not results.multi_hand_landmarks:
            return None

        # Compute bounding box from hand landmarks
        landmarks = results.multi_hand_landmarks[0]
        xs = [lm.x * w for lm in landmarks.landmark]
        ys = [lm.y * h for lm in landmarks.landmark]

        x_min, x_max = int(min(xs)), int(max(xs))
        y_min, y_max = int(min(ys)), int(max(ys))

        # Add padding
        bbox_w = x_max - x_min
        bbox_h = y_max - y_min
        pad_w = int(bbox_w * self.padding_ratio)
        pad_h = int(bbox_h * self.padding_ratio)

        x_min = max(0, x_min - pad_w)
        y_min = max(0, y_min - pad_h)
        x_max = min(w, x_max + pad_w)
        y_max = min(h, y_max + pad_h)

        # Make square (centered)
        bbox_w = x_max - x_min
        bbox_h = y_max - y_min
        bbox_size = max(bbox_w, bbox_h)

        x_center = (x_min + x_max) // 2
        y_center = (y_min + y_max) // 2

        x1 = max(0, x_center - bbox_size // 2)
        y1 = max(0, y_center - bbox_size // 2)
        x2 = min(w, x1 + bbox_size)
        y2 = min(h, y1 + bbox_size)

        crop = rgb[y1:y2, x1:x2]
        return Image.fromarray(crop)

    def preprocess_directory(self, input_dir, output_dir):
        """Apply hand detection to all extracted frames"""
        input_dir = Path(input_dir)
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        stats = {'processed': 0, 'no_hand': 0}

        letter_dirs = [d for d in input_dir.iterdir() if d.is_dir()]

        for letter_dir in tqdm(letter_dirs, desc="Preprocessing with MediaPipe"):
            label = letter_dir.name
            output_label_dir = output_dir / label
            output_label_dir.mkdir(exist_ok=True)

            for img_path in letter_dir.glob('*.jpg'):
                image = cv2.imread(str(img_path))
                if image is None:
                    continue

                crop = self.detect_and_crop(image)

                if crop is not None:
                    crop_resized = crop.resize((self.image_size, self.image_size))
                    crop_resized.save(output_label_dir / img_path.name)
                    stats['processed'] += 1
                else:
                    stats['no_hand'] += 1

        print(f"\nâœ“ Preprocessing complete:")
        print(f"  Processed: {stats['processed']} frames")
        print(f"  Skipped (no hand): {stats['no_hand']} frames")

        return stats

print("âœ“ MediaPipeHandCropper class defined")

!pip install mediapipe==0.10.13

# Apply MediaPipe preprocessing to extracted frames
cropper = MediaPipeHandCropper(image_size=IMAGE_SIZE, padding_ratio=0.15)
cropping_stats = cropper.preprocess_directory(
    input_dir='/content/wlasl_letters_raw',
    output_dir='/content/wlasl_letters_cropped'
)

"""## 5. Mixed Dataset Creation"""

class MixedASLDataset(Dataset):
    """Combine clean alphabet data with noisy WLASL letter frames"""

    def __init__(self, clean_dir, noisy_dir, transform, clean_ratio=0.7):
        self.clean_ds = ImageFolder(clean_dir, transform=transform)
        self.noisy_ds = ImageFolder(noisy_dir, transform=transform)

        # Verify class alignment
        if self.clean_ds.classes != self.noisy_ds.classes:
            print("âš  WARNING: Class mismatch detected!")
            print(f"Clean classes: {self.clean_ds.classes}")
            print(f"Noisy classes: {self.noisy_ds.classes}")
            raise ValueError("Class labels must match between datasets")

        self.classes = self.clean_ds.classes

        # Calculate sampling to achieve target ratio
        total_clean = len(self.clean_ds)
        total_noisy = len(self.noisy_ds)

        self.clean_samples = total_clean
        self.noisy_samples = int(total_clean * (1 - clean_ratio) / clean_ratio)

        print(f"âœ“ Mixed Dataset: {self.clean_samples} clean + {self.noisy_samples} noisy samples")

    def __len__(self):
        return self.clean_samples + self.noisy_samples

    def __getitem__(self, idx):
        if idx < self.clean_samples:
            return self.clean_ds[idx]
        else:
            # Wrap around noisy dataset with modulo
            noisy_idx = (idx - self.clean_samples) % len(self.noisy_ds)
            return self.noisy_ds[noisy_idx]

print("âœ“ MixedASLDataset class defined")

# Define transforms (aggressive augmentation for robustness)
train_transform_robust = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),

    # Geometric augmentation (more aggressive)
    transforms.RandomAffine(
        degrees=15,
        translate=(0.1, 0.1),
        scale=(0.85, 1.15),
        shear=5
    ),
    transforms.RandomHorizontalFlip(p=0.3),

    # Color augmentation (simulate lighting)
    transforms.ColorJitter(
        brightness=0.3,
        contrast=0.3,
        saturation=0.2,
        hue=0.05
    ),

    # Simulate blur (motion blur during webcam capture)
    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5)),

    transforms.ToTensor(),

    # Add noise
    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.02),

    # Standard normalization
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
])

val_transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
])

print("âœ“ Transforms defined")

# Create mixed training dataset
mixed_train_ds = MixedASLDataset(
    clean_dir=f"{grassknoted_asl_alphabet_path}/asl_alphabet_train/asl_alphabet_train",
    noisy_dir='/content/wlasl_letters_cropped',
    transform=train_transform_robust,
    clean_ratio=CLEAN_RATIO
)

# Create validation dataset (clean data only)
val_ds = ImageFolder(
    f"{grassknoted_asl_alphabet_path}/asl_alphabet_train/asl_alphabet_train",
    transform=val_transform
)

# Create dataloaders
mixed_train_loader = DataLoader(
    mixed_train_ds,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=NUM_WORKERS,
    pin_memory=True
)

val_loader = DataLoader(
    val_ds,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=NUM_WORKERS,
    pin_memory=True
)

print(f"âœ“ Dataloaders created")
print(f"  Training batches: {len(mixed_train_loader)}")
print(f"  Validation batches: {len(val_loader)}")

import os

print(f"Contents of WLASL path ({wlasl_path}):")
for root, dirs, files in os.walk(wlasl_path):
    level = root.replace(wlasl_path, '').count(os.sep)
    indent = ' ' * 4 * (level)
    print(f'{indent}{os.path.basename(root)}/')
    subindent = ' ' * 4 * (level + 1)
    for f in files:
        if f.endswith('.mp4') or f.endswith('.json') or f.endswith('.txt'): # Limit output for readability
            print(f'{subindent}{f}')
    if level >= 2: # Only show up to 2 levels deep to avoid excessive output
        del dirs[:] # Don't go deeper

print("Re-running WLASLLetterExtractor class definition...")
# Re-running the cell will reload the class definition with the fix
class WLASLLetterExtractor:
    """Extract letter-only frames from WLASL videos"""

    LETTER_GLOSSES = set('ABCDEFGHIJKLMNOPQRSTUVWXYZ')

    def __init__(self, wlasl_root, frame_interval=4):
        self.wlasl_root = Path(wlasl_root)
        self.frame_interval = frame_interval

    def find_letter_videos(self):
        """Scan WLASL directory structure for letter videos"""
        letter_videos = []

        # Try common WLASL directory structures
        possible_dirs = [
            self.wlasl_root / 'videos',
            self.wlasl_root / 'WLASL2000',
            self.wlasl_root / 'dataset' / 'SL', # Added this path
            self.wlasl_root,
        ]

        videos_dir = None
        for dir_path in possible_dirs:
            if dir_path.exists():
                videos_dir = dir_path
                break

        if videos_dir is None:
            print("âš  Could not find videos directory")
            return []

        print(f"Scanning: {videos_dir}")

        # Look for letter subdirectories
        for item in videos_dir.iterdir():
            if not item.is_dir():
                continue

            gloss = item.name.upper()

            # Only process single-letter glosses
            if gloss in self.LETTER_GLOSSES:
                for video_path in item.glob('*.mp4'):
                    letter_videos.append({
                        'video_path': video_path,
                        'label': gloss
                    })

        return letter_videos

    def extract_frames_from_video(self, video_path, max_frames=20):
        """Extract frames from middle 60% of video (avoid transitions)"""
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            return []

        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # Skip first and last 20% to avoid hand entering/leaving frame
        start_frame = int(total_frames * 0.2)
        end_frame = int(total_frames * 0.8)

        frames = []
        frame_idx = 0

        while cap.isOpened() and len(frames) < max_frames:
            ret, frame = cap.read()
            if not ret:
                break

            # Extract from stable middle portion
            if start_frame <= frame_idx <= end_frame:
                if (frame_idx - start_frame) % self.frame_interval == 0:
                    frames.append(frame)

            frame_idx += 1

        cap.release()
        return frames

    def process_all(self, output_dir, max_frames_per_video=20):
        """Extract all letter frames and save to disk"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        letter_videos = self.find_letter_videos()

        if not letter_videos:
            print("âš  No letter videos found. Check WLASL directory structure.")
            return {}

        stats = defaultdict(int)

        print(f"Found {len(letter_videos)} letter videos. Extracting frames...")

        for video_info in tqdm(letter_videos, desc="Extracting frames"):
            label = video_info['label']
            video_path = video_info['video_path']

            frames = self.extract_frames_from_video(video_path, max_frames_per_video)

            if not frames:
                continue

            # Save frames
            label_dir = output_dir / label
            label_dir.mkdir(exist_ok=True)

            video_id = video_path.stem
            for idx, frame in enumerate(frames):
                frame_name = f"{video_id}_frame{idx:03d}.jpg"
                cv2.imwrite(str(label_dir / frame_name), frame)
                stats[label] += 1

        print("\nâœ“ Frame extraction complete:")
        for letter in sorted(stats.keys()):
            print(f"  {letter}: {stats[letter]} frames")

        return stats

print("âœ“ WLASLLetterExtractor class defined")

# Extract letter frames from WLASL
extractor = WLASLLetterExtractor(wlasl_path, frame_interval=FRAME_INTERVAL)
extraction_stats = extractor.process_all(
    output_dir='/content/wlasl_letters_raw',
    max_frames_per_video=MAX_FRAMES_PER_VIDEO
)

# Apply MediaPipe preprocessing to extracted frames
cropper = MediaPipeHandCropper(image_size=IMAGE_SIZE, padding_ratio=0.15)
cropping_stats = cropper.preprocess_directory(
    input_dir='/content/wlasl_letters_raw',
    output_dir='/content/wlasl_letters_cropped'
)

"""## 6. Model Definition"""

def build_model(num_classes):
    """Build ResNet-18 model (matches your original architecture)"""
    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
    in_features = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Dropout(0.3),
        nn.Linear(in_features, num_classes)
    )
    return model

print("âœ“ Model builder function defined")

"""## 7. Training Functions"""

def train_one_epoch(model, loader, criterion, optimizer, scaler, epoch):
    """Train for one epoch"""
    model.train()
    total_loss = 0.0

    pbar = tqdm(loader, desc=f"Epoch {epoch:02d} [train]", leave=False)
    for x, y in pbar:
        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)
        optimizer.zero_grad(set_to_none=True)

        with torch.amp.autocast(device_type="cuda", enabled=(DEVICE == "cuda")):
            out = model(x)
            loss = criterion(out, y)

        scaler.scale(loss).backward()
        nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()
        pbar.set_postfix(loss=f"{loss.item():.4f}")

    pbar.close()
    return total_loss / len(loader)

@torch.no_grad()
def eval_one_epoch(model, loader, criterion):
    """Evaluate on validation set"""
    model.eval()
    total_loss, correct, total = 0.0, 0, 0

    pbar = tqdm(loader, desc="[val]", leave=False)
    for x, y in pbar:
        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)

        with torch.amp.autocast(device_type="cuda", enabled=(DEVICE == "cuda")):
            out = model(x)
            loss = criterion(out, y)

        total_loss += loss.item()
        pred = out.argmax(1)
        correct += (pred == y).sum().item()
        total += y.size(0)

        acc = correct / total
        pbar.set_postfix(loss=f"{loss.item():.4f}", acc=f"{acc:.3f}")

    pbar.close()
    return total_loss / len(loader), correct / total

@torch.no_grad()
def evaluate_per_class(model, loader, class_names):
    """Compute per-class accuracy to identify weak letters"""
    model.eval()

    class_correct = defaultdict(int)
    class_total = defaultdict(int)

    for x, y in tqdm(loader, desc="Per-class eval"):
        x, y = x.to(DEVICE), y.to(DEVICE)
        out = model(x)
        pred = out.argmax(1)

        for true_label, pred_label in zip(y.cpu().numpy(), pred.cpu().numpy()):
            class_total[true_label] += 1
            if true_label == pred_label:
                class_correct[true_label] += 1

    print("\nPer-Class Accuracy:")
    print("=" * 40)
    for class_idx in sorted(class_total.keys()):
        acc = class_correct[class_idx] / class_total[class_idx]
        print(f"{class_names[class_idx]:>10s}: {acc:6.2%} ({class_correct[class_idx]}/{class_total[class_idx]})")

    overall_acc = sum(class_correct.values()) / sum(class_total.values())
    print("=" * 40)
    print(f"{'Overall':>10s}: {overall_acc:6.2%}")

    return overall_acc

print("âœ“ Training functions defined")

"""## 8. Load Pre-trained Model"""

# Upload your pre-trained model
from google.colab import files

print("Please upload your asl_resnet18_best.pth file:")
uploaded = files.upload()

# Verify the file was uploaded
if 'asl_resnet18_best.pth' in uploaded:
    print("âœ“ Model file uploaded successfully")
else:
    print("âš  Warning: Expected 'asl_resnet18_best.pth' but got:", list(uploaded.keys()))

# Load the pre-trained model
checkpoint = torch.load('asl_resnet18_best.pth', map_location=DEVICE)
model = build_model(NUM_CLASSES).to(DEVICE)
model.load_state_dict(checkpoint['model_state'])

print(f"âœ“ Loaded model from epoch {checkpoint['epoch']}")
print(f"  Best validation loss: {checkpoint['best_val_loss']:.4f}")
print(f"  Classes: {checkpoint['classes'][:5]}... (showing first 5)")

"""## 9. Fine-Tuning Setup"""

# Setup training components
criterion = nn.CrossEntropyLoss(label_smoothing=0.05)
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=FINETUNE_EPOCHS)
scaler = torch.amp.GradScaler(enabled=(DEVICE == "cuda"))

print("âœ“ Training setup complete")
print(f"  Optimizer: AdamW (lr={LEARNING_RATE})")
print(f"  Scheduler: CosineAnnealingLR")
print(f"  Loss: CrossEntropyLoss with label smoothing")

"""## 10. Fine-Tuning Training Loop"""

# Fine-tune the model
best_val_loss = checkpoint['best_val_loss']
train_losses = []
val_losses = []
val_accs = []

for epoch in range(1, FINETUNE_EPOCHS + 1):
    print(f"\n{'='*60}")
    print(f"Fine-Tune Epoch {epoch:02d}/{FINETUNE_EPOCHS}")
    print('='*60)

    tr_loss = train_one_epoch(model, mixed_train_loader, criterion, optimizer, scaler, epoch)
    va_loss, va_acc = eval_one_epoch(model, val_loader, criterion)
    scheduler.step()

    train_losses.append(tr_loss)
    val_losses.append(va_loss)
    val_accs.append(va_acc)

    print(f"\nEpoch {epoch:02d} Summary:")
    print(f"  Train Loss: {tr_loss:.4f}")
    print(f"  Val Loss:    {va_loss:.4f}")
    print(f"  Val Acc:     {va_acc:.4f}")

    # Save if improved
    if va_loss < best_val_loss:
        best_val_loss = va_loss
        torch.save({
            'model_state': model.state_dict(),
            'classes': checkpoint['classes'],
            'image_size': IMAGE_SIZE,
            'best_val_loss': best_val_loss,
            'epoch': epoch,
        }, 'asl_resnet18_finetuned.pth')
        print(f"  âœ“ New best model saved! (val loss: {best_val_loss:.4f})")

print("\n" + "="*60)
print("âœ“ Fine-tuning complete!")
print("="*60)

"""## 11. Visualization & Analysis"""

# Plot training curves
epochs = range(1, len(train_losses) + 1)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Loss plot
ax1.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)
ax1.plot(epochs, val_losses, 'r-', label='Val Loss', linewidth=2)
ax1.set_xlabel('Epoch', fontsize=12)
ax1.set_ylabel('Loss', fontsize=12)
ax1.set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')
ax1.legend(fontsize=11)
ax1.grid(True, alpha=0.3)

# Accuracy plot
ax2.plot(epochs, val_accs, 'g-', label='Val Accuracy', linewidth=2)
ax2.set_xlabel('Epoch', fontsize=12)
ax2.set_ylabel('Accuracy', fontsize=12)
ax2.set_title('Validation Accuracy', fontsize=14, fontweight='bold')
ax2.legend(fontsize=11)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ“ Training curves saved as 'training_curves.png'")

"""## 12. Detailed Evaluation"""

# Per-class accuracy evaluation
print("\n" + "="*60)
print("FINAL EVALUATION - Per-Class Performance")
print("="*60)

overall_acc = evaluate_per_class(model, val_loader, checkpoint['classes'])

"""## 13. Download Fine-Tuned Model"""

# Download the fine-tuned model
from google.colab import files

print("Downloading fine-tuned model...")
files.download('asl_resnet18_finetuned.pth')

print("\nâœ“ Fine-tuned model downloaded!")
print("\nYou can now use this model in your webcam inference pipeline.")
print("Expected improvements:")
print("  - Better robustness to lighting variations")
print("  - More stable predictions with motion blur")
print("  - Improved performance with diverse backgrounds")

"""## 14. Summary & Next Steps

### What We Did:
1. âœ“ Extracted letter-only frames from WLASL videos
2. âœ“ Applied MediaPipe hand detection (matching inference pipeline)
3. âœ“ Mixed clean alphabet data with noisy WLASL frames
4. âœ“ Fine-tuned existing ResNet-18 with aggressive augmentation
5. âœ“ Evaluated per-class performance

### Expected Behavior:
- **Validation accuracy may drop slightly** (e.g., 99% â†’ 94%) - this is expected!
- The model trades some clean-data performance for real-world robustness
- **Webcam performance should improve** through better handling of:
  - Motion blur
  - Variable lighting
  - Different backgrounds
  - Hand position variations

### Testing Your Model:
1. Replace your webcam inference model with `asl_resnet18_finetuned.pth`
2. Test each letter 5-10 times in different conditions
3. Note prediction stability (less flickering between letters)
4. Check particularly challenging letters (M/N, U/V, etc.)

### If Results Aren't Satisfactory:
- **Increase noisy ratio:** Change `CLEAN_RATIO` to 0.5 (50/50 split)
- **More augmentation:** Add stronger blur or brightness variations
- **More WLASL frames:** Increase `MAX_FRAMES_PER_VIDEO` to 30-40
- **Longer training:** Increase `FINETUNE_EPOCHS` to 15-20

### Academic Justification:
This approach addresses the **domain gap** between clean training data and noisy real-world conditions by:
- Incorporating realistic variations without changing task definition
- Maintaining frame-based architecture (no sequence modeling needed)
- Using identical preprocessing at train and test time
- Accepting slight validation accuracy drop for practical robustness gains

**Good luck with your project!** ðŸš€
"""