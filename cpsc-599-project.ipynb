{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":29550,"sourceType":"datasetVersion","datasetId":23079}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision import transforms, models\nfrom tqdm.auto import tqdm\n\nfrom typing import List, Tuple","metadata":{"_uuid":"51b7c015-8cc5-4327-ac28-7d18d62ebc16","_cell_guid":"52e92fe4-aa74-451e-b932-57545cf48c70","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-01T23:41:50.996299Z","iopub.execute_input":"2026-01-01T23:41:50.996480Z","iopub.status.idle":"2026-01-01T23:42:00.616139Z","shell.execute_reply.started":"2026-01-01T23:41:50.996463Z","shell.execute_reply":"2026-01-01T23:42:00.615511Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nNUM_CLASSES_LETTERS = 29      # ASL alphabet\nNUM_CLASSES_WORDS = 2000      # WLASL (if used) - could not implement this time \nDATA_ROOT = \"/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train\"\nTEST_ROOT = \"/kaggle/input/asl-alphabet/asl_alphabet_test/asl_alphabet_test\"\nIMAGE_SIZE = 224\nLEARNING_RATE = 1e-4\nEPOCHS = 20\nBATCH_SIZE = 64\nNUM_WORKERS = 2","metadata":{"_uuid":"a15c8978-a2cd-4b3a-bc55-1e0117d20db9","_cell_guid":"7dc00e33-939a-4f9b-b9bd-42d0043900c4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-01T23:42:00.617241Z","iopub.execute_input":"2026-01-01T23:42:00.617525Z","iopub.status.idle":"2026-01-01T23:42:00.708124Z","shell.execute_reply.started":"2026-01-01T23:42:00.617507Z","shell.execute_reply":"2026-01-01T23:42:00.707061Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Enhanced Data Augmentation\n\nTo improve generalization and prevent overfitting, we applied an enhanced data‚Äêaugmentation pipeline.  \nThe goal of augmentation is to artificially increase dataset diversity by creating varied versions of the same image. Although our presented data set is quite diverse, this additional step prevents the model from memorizing training examples and encourages learning more robust, invariant features.\n\n### **Transformations Used** \n- Random rotations  \n- Color jitter (hue, saturation, brightness variations)  \n- Random resized crops  \n- Normalization with ImageNet mean and std  \n\n### **Why This Helps**\nDeep CNNs such as ResNet can easily overfit when trained on datasets with limited intra-class variation. Augmentation simulates real-world variability and forces the network to learn spatially invariant, shape-based representations rather than memorizing texture patterns.\n\n### **Observed Improvements**\n- Reduced gap between training and validation accuracy  \n- Smoother training curves with less volatility  \n- Higher validation accuracy due to improved generalization  \n","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.RandomApply([\n        transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.15, hue=0.02)\n    ], p=0.8),\n    transforms.RandomAffine(degrees=10, translate=(0.08, 0.08), scale=(0.9, 1.1)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])","metadata":{"_uuid":"834103ff-6a0f-40c2-8d65-b29785470913","_cell_guid":"c4941c66-6e84-49ef-8d3c-a7feba1afb43","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-01T23:42:00.709140Z","iopub.execute_input":"2026-01-01T23:42:00.709473Z","iopub.status.idle":"2026-01-01T23:42:00.749838Z","shell.execute_reply.started":"2026-01-01T23:42:00.709439Z","shell.execute_reply":"2026-01-01T23:42:00.749174Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Splitting (Train / Validation)\n\nWe load the dataset twice using `ImageFolder`, applying **different transforms** for training and validation.  \nThis allows us to use **data augmentation** during training (e.g., random crops, flips) while keeping the validation data deterministic and representative of real inference conditions.\n\nTo ensure **reproducibility**, we generate a random permutation of dataset indices using a fixed random seed (`42`).  \nThis guarantees that the same images are assigned to the training and validation sets across runs.\n\nWe then split the dataset as follows:\n- **90%** of the data is used for training  \n- **10%** of the data is reserved for validation  \n\nThe split is implemented using `Subset`, which indexes into the original dataset without duplicating data.\n\nAlthough both subsets reference the same underlying images, they apply **different transforms**, ensuring:\n- No data leakage between training and validation\n- Consistent experimental results\n- Proper separation of augmented training data from clean validation data\n","metadata":{}},{"cell_type":"code","source":"\n# DATASET SPLITTING\nfull_train_ds = ImageFolder(DATA_ROOT, transform=train_transform)\nfull_val_ds   = ImageFolder(DATA_ROOT, transform=val_transform)\n\n# reproducible split indices\nval_frac = 0.1\nn = len(full_train_ds)\nindices = torch.randperm(n, generator=torch.Generator().manual_seed(42)).tolist()\nval_size = int(n * val_frac)\n\nval_idx = indices[:val_size]\ntrain_idx = indices[val_size:]\n\ntrain_ds = Subset(full_train_ds, train_idx)\nval_ds   = Subset(full_val_ds,   val_idx)\n\nprint(\"Total:\", n)\nprint(\"Train:\", len(train_ds), \"Val:\", len(val_ds))\nprint(\"Classes:\", len(full_train_ds.classes), full_train_ds.classes[:10])\n","metadata":{"_uuid":"0208063e-2a0e-40e4-bd85-9e1aa16941a1","_cell_guid":"58dcc212-338e-492b-b3d2-a0c36c3f220f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-01T23:42:00.751291Z","iopub.execute_input":"2026-01-01T23:42:00.751497Z","iopub.status.idle":"2026-01-01T23:44:19.191590Z","shell.execute_reply.started":"2026-01-01T23:42:00.751482Z","shell.execute_reply":"2026-01-01T23:44:19.190597Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS,\n    pin_memory=True,\n    persistent_workers=True,\n    prefetch_factor=2\n)\n\nval_loader = DataLoader(\n    val_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True,\n    persistent_workers=True,\n    prefetch_factor=2\n)\n\n# shows us size\nx, y = next(iter(train_loader))\nprint(\"Batch:\", x.shape, y.shape, \"dtype:\", x.dtype)\n","metadata":{"_uuid":"46b7483c-03a7-4180-9eee-c29bc1758c2e","_cell_guid":"7b21745c-fdeb-4f57-9923-73addf416371","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-01T23:44:19.192488Z","iopub.execute_input":"2026-01-01T23:44:19.192759Z","iopub.status.idle":"2026-01-01T23:44:20.348171Z","shell.execute_reply.started":"2026-01-01T23:44:19.192734Z","shell.execute_reply":"2026-01-01T23:44:20.346944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Architecture (ResNet-18)\n\nWe use **ResNet-18**, a deep convolutional neural network pretrained on the ImageNet dataset, as the backbone of our model.  \nLeveraging a pretrained model allows us to benefit from learned low-level and mid-level visual features, improving convergence speed and performance‚Äîespecially with limited training data.\n\n### Model Construction\n- The model is initialized with **ImageNet pretrained weights**.\n- We extract the number of input features to the final fully connected layer (`model.fc.in_features`).\n- The original classification head is replaced with a custom head consisting of:\n  - `Dropout(p = 0.3)` to reduce overfitting\n  - A `Linear` layer mapping to `num_classes`, matching the target classification task\n\n### Why This Design?\n- **Transfer learning**: Reuses robust visual features learned from large-scale data.\n- **Regularization**: Dropout improves generalization.\n- **Flexibility**: The function allows easy reuse for different numbers of output classes.\n\nThe resulting model is well-suited for fine-tuning on a custom image classification dataset.\n","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision import models\n\n# Using the resNet18 model architecture\ndef build_model(num_classes):\n    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n    in_features = model.fc.in_features\n    model.fc = nn.Sequential(\n        nn.Dropout(0.3),\n        nn.Linear(in_features, num_classes)\n    )\n    return model","metadata":{"_uuid":"01779556-51aa-4533-9fe5-2abca733b5f1","_cell_guid":"8a1c57c1-c142-4f64-a5f3-2b798cc39153","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-01T23:44:20.349774Z","iopub.execute_input":"2026-01-01T23:44:20.350075Z","iopub.status.idle":"2026-01-01T23:44:20.357334Z","shell.execute_reply.started":"2026-01-01T23:44:20.350041Z","shell.execute_reply":"2026-01-01T23:44:20.356068Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training + Evaluation Loop (Mixed Precision)\n\nThis section sets up the full training pipeline: model initialization, loss/optimizer/scheduler configuration, and the per-epoch **train -> validate** loop.\n\n### Loss Function\nWe use **Cross Entropy Loss** with **label smoothing**:\n- `nn.CrossEntropyLoss(label_smoothing=0.05)`\nLabel smoothing slightly softens the target labels, which can reduce overconfidence and improve generalization.\n\n### Optimizer + LR Scheduler\n- **AdamW** is used for optimization (`lr=3e-4`, `weight_decay=1e-4`), which typically works well for fine-tuning CNNs.\n- A **Cosine Annealing** learning rate schedule is applied over `T_max=15` epochs, gradually reducing the learning rate in a smooth cosine curve.\n---\n\n## üîÅ `train_one_epoch`\nDuring training we:\n1. Set the model to training mode with `model.train()`.\n2. Iterate over batches with a `tqdm` progress bar.\n3. Move inputs/labels to the target device (with `non_blocking=True` for faster transfers).\n4. Zero gradients using `optimizer.zero_grad(set_to_none=True)` (more memory-efficient).\n5. Forward pass + loss computation under `autocast(...)`.\n6. Backprop using scaled gradients (`scaler.scale(loss).backward()`).\n7. Clip gradients to stabilize training (`clip_grad_norm_`).\n8. Update model weights with `scaler.step(optimizer)` and update the scaler.\n\nWe track and return the **average training loss** over the epoch.\n\n---\n\n## `eval_one_epoch` (Validation)\nValidation is run after each epoch to measure generalization performance.\n\nKey differences from training:\n- `@torch.no_grad()` disables gradient tracking (faster + lower memory).\n- `model.eval()` switches off training-only behaviors (e.g., Dropout, BatchNorm updates).\n- We compute:\n  - average validation loss\n  - validation accuracy (`correct / total`)\n\nEven though gradients are disabled, we still use `autocast(...)` on CUDA for faster inference.\n\n---\n\n## Training Loop + Checkpointing\nFor each epoch (1 to 15), we:\n1. Train for one epoch\n2. Evaluate on validation data\n3. Step the LR scheduler\n4. Store loss/accuracy history for plotting later\n\n### Saving the Best Model\nWe track `best_val_loss` and save a checkpoint whenever validation loss improves:\n- model weights (`model_state`)\n- class labels (`classes`)\n- image size (`image_size`)\n- best validation loss and epoch\n\nCheckpoint file:\n- `asl_resnet18_best.pth`\n\nThis ensures we keep the model that generalizes best, not just the one from the final epoch.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.amp import autocast, GradScaler  # <-- new API\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = build_model(NUM_CLASSES_LETTERS).to(DEVICE)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.05)\noptimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n\nscaler = GradScaler(enabled=(DEVICE == \"cuda\"))\n\ndef train_one_epoch(model, loader, epoch):\n    model.train()\n    total_loss = 0.0\n\n    pbar = tqdm(loader, desc=f\"Epoch {epoch:02d} [train]\", leave=False)\n    try:\n        for x, y in pbar:\n            x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n\n            with autocast(device_type=\"cuda\", enabled=(DEVICE == \"cuda\")):\n                out = model(x)\n                loss = criterion(out, y)\n\n            scaler.scale(loss).backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n\n            total_loss += loss.item()\n            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n    finally:\n        pbar.close()\n\n    return total_loss / len(loader)\n# use no grad as we want inferences in a tighter time loop through our webcam, this eval step is performed after training one epoch\n@torch.no_grad()\ndef eval_one_epoch(model, loader):\n    model.eval()\n    total_loss, correct, total = 0.0, 0, 0\n\n    pbar = tqdm(loader, desc=\"           [val]\", leave=False)\n    try:\n        for x, y in pbar:\n            x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n\n            with autocast(device_type=\"cuda\", enabled=(DEVICE == \"cuda\")):\n                out = model(x)\n                loss = criterion(out, y)\n\n            total_loss += loss.item()\n            pred = out.argmax(1)\n            correct += (pred == y).sum().item()\n            total += y.size(0)\n\n            acc = correct / total\n            pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{acc:.3f}\")\n    finally:\n        pbar.close()\n\n    return total_loss / len(loader), correct / total\n\n\n\nbest_val_loss = float(\"inf\")\ntrain_losses = []\nval_losses = []\nval_accs = []\n\n\nfor epoch in range(1, 16):\n    print(f\"\\n=== Epoch {epoch:02d} ===\")\n\n    tr_loss = train_one_epoch(model, train_loader, epoch)\n    va_loss, va_acc = eval_one_epoch(model, val_loader)\n    scheduler.step()\n\n    print(\n        f\"Epoch {epoch:02d} | \"\n        f\"train loss {tr_loss:.4f} | \"\n        f\"val loss {va_loss:.4f} | \"\n        f\"val acc {va_acc:.4f}\"\n    )\n\n    train_losses.append(tr_loss)\n    val_losses.append(va_loss)\n    val_accs.append(va_acc)\n\n    if va_loss < best_val_loss:\n        best_val_loss = va_loss\n        torch.save({\n            \"model_state\": model.state_dict(),\n            \"classes\": full_train_ds.classes,\n            \"image_size\": IMAGE_SIZE,\n            \"best_val_loss\": best_val_loss,\n            \"epoch\": epoch,\n        }, \"asl_resnet18_best.pth\")\n        print(f\"saved new best val loss: {best_val_loss:.4f}\")\n\n","metadata":{"_uuid":"fb4b3a1e-6f3e-4014-ae16-3677bc940878","_cell_guid":"fca002a0-287e-4eeb-9bfa-b2f86ebd2308","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-01T23:49:40.851573Z","iopub.execute_input":"2026-01-01T23:49:40.851886Z","iopub.status.idle":"2026-01-02T01:16:26.819107Z","shell.execute_reply.started":"2026-01-01T23:49:40.851865Z","shell.execute_reply":"2026-01-02T01:16:26.818301Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Curves (Loss & Accuracy)\n\nTo analyze model performance over time, we visualize both **training loss**, **validation loss**, and **validation accuracy** across epochs.\n\n### Loss Curves\n- **Training loss** shows how well the model fits the training data.\n- **Validation loss** reflects how well the model generalizes to unseen data.\n- Plotting both together helps diagnose:\n  - Overfitting (training loss decreases while validation loss increases)\n  - Underfitting (both losses remain high)\n  - Proper convergence (both decrease and stabilize)\n\n### Accuracy Curve\n- Validation accuracy measures classification performance on unseen data.\n- Tracking accuracy over epochs allows us to confirm that improvements in loss translate to better predictive performance.\n\n### Why This Matters\nVisualizing these metrics helps:\n- Validate training stability\n- Select the best epoch (in conjunction with checkpointing)\n- Debug learning rate or regularization issues\n\nThese plots provide an intuitive summary of the model‚Äôs learning behavior throughout training.\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nepochs = range(1, len(train_losses) + 1)\n\n# ---- Loss plot ----\nplt.figure()\nplt.plot(epochs, train_losses, label=\"Train Loss\")\nplt.plot(epochs, val_losses, label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend()\nplt.show()\n\n# ---- Accuracy plot ----\nplt.figure()\nplt.plot(epochs, val_accs, label=\"Val Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Validation Accuracy\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
